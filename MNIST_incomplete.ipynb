{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAIS++ Logistic Regression Workshop!\n",
    "\n",
    "Before you go through this code, make sure you read [lesson 3]('http://caisplusplus.usc.edu/blog/curriculum/lesson3') from our curriculum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Import Dataset & Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Statemements\n",
    "\n",
    "import random\n",
    "import pickle # used to save and restore python objects\n",
    "import gzip\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "# Load the Dataset\n",
    "\n",
    "f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "train_set, valid_set, test_set = pickle.load(f, encoding = 'latin1')\n",
    "f.close()\n",
    "\n",
    "# What does our dataset look like?\n",
    "print(type(train_set))\n",
    "print(type(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Inputs shape is (50000, 784)\n",
      "Input type is <class 'numpy.ndarray'>\n",
      "Labels:\n",
      "[5 0 4 ... 8 4 8]\n",
      "Labels shape is(50000,)\n",
      "Labels type is <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# A Closer Look at Our Dataset\n",
    "\n",
    "print(\"Inputs:\")\n",
    "print(train_set[0])\n",
    "\n",
    "# 50,000 MNIST images: each represented as a vector of length 784 (28 x 28).\n",
    "# 0 corresponds to a dark pixel, 1 corresponds to a light pixel\n",
    "print(\"Inputs shape is \" + str(train_set[0].shape)) \n",
    "print(\"Input type is \" + str(type(train_set[0])))\n",
    "print(\"Labels:\")\n",
    "\n",
    "# There's 50,000 labels (one for each example)\n",
    "print(train_set[1])\n",
    "print(\"Labels shape is\" + str(train_set[1].shape))\n",
    "print(\"Labels type is \" + str(type(train_set[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: to_categorical -- One-Hot Vector Encoding \n",
    "\n",
    "# Converts class labels (integers from 0 to nb_classes) to one-hot vector\n",
    "# Example: 5 => [0 0 0 0 0 1 0 0 0 0]\n",
    "\n",
    "# Arguments:\n",
    "    # y: array, class labels to convert\n",
    "    # nb_classes: integer, total number of classes \n",
    "\n",
    "def to_categorical(y, nb_classes):\n",
    "    y = np.asarray(y, dtype='int32')\n",
    "    if not nb_classes:\n",
    "        nb_classes = np.max(y)+1\n",
    "    answer = np.zeros((len(y), nb_classes))\n",
    "    answer[np.arange(len(y)),y] = 1.\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Train-Test Split, One-Hot Encoding for our data\n",
    "\n",
    "train_x = train_set[0]\n",
    "train_y = to_categorical(train_set[1], 10)\n",
    "test_x = test_set[0]\n",
    "test_y = to_categorical(test_set[1],10)\n",
    "\n",
    "# Result of One-Hot Encoding Class Labels \n",
    "print(test_y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Create Logistic Regression Model using TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.tensorflow.org/versions/r1.1/images/softmax-regression-scalargraph.png\" style=\"width: 600px;\"/>\n",
    "<br />\n",
    "<br />\n",
    "<img src=\"https://www.tensorflow.org/versions/r1.1/images/softmax-regression-vectorequation.png\" style=\"width: 600px;\"/>\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "\n",
    "$$y = \\text{softmax}(Wx + b)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Model Variables in TensorFlow\n",
    "\n",
    "# This just helps with using tensorflow inside jupyter (reset/clear all tf variables)\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Weights Variable (xavier initializer -- random values centered around zero)\n",
    "W = tf.get_variable(\"W\", shape=[784, 10], initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "# Biases Variable (initialized to zero)\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "### TODO: Create Model Placeholders for x and y_\n",
    "\n",
    "# x = Input Parameter\n",
    "# y_ = Actual Labels (the one-hot vector with digit labels)\n",
    "\n",
    "x = tf.placeholder(tf.float32 , shape=(None, 784))\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Define hypothesis fxn (y)\n",
    "\n",
    "# Hint: Use tf.nn.softmax() and tf.matmul() functions to matrix multiply x and W, then add b\n",
    "y = tf.nn.softmax(tf.matmul(x, W)+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Training the Model\n",
    "\n",
    "Using the given training examples, our goal is to find the best possible hypothesis function. In other words, we need to find the weights that minimize the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Cost Function: Cross-Entropy Loss\n",
    "\n",
    "# cost increases as predicted probability diverges from actual label \n",
    "loss = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization -- Batch Gradient Descent\n",
    "\n",
    "# At each step, we'll use a random subset from the training set\n",
    "# Instead of traditional gradient descent, which looks at every example at every step \n",
    " \n",
    "# Batch Gradient Descent\n",
    "    \n",
    "# Function: generate_batch: takes in desired batch size and returns a \"batch\" of data (MNIST images)\n",
    "def generate_batch(batch_size):\n",
    "    ### TODO: complete the \"indexes = \" line, where we want to randomly choose a set of images from the training set\n",
    "    # The size of this set should be equal to the batch_size\n",
    "    # Hint: use the random.sample() function to intialize the \"indexes\" variable.\n",
    "    \n",
    "    indexes = random.sample(range(50000), batch_size)\n",
    "    \n",
    "    # We'll return the images from train_x that correspond to the indexes in this \"batch\"\n",
    "    return train_x[indexes], train_y[indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Isaac\\Miniconda3\\envs\\caispp\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1645: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "# Set up Training in TensorFlow\n",
    "\n",
    "### TODO: Decide model hyperparameters \n",
    "learning_rate = .032\n",
    "iterations = 5000\n",
    "batch_size = 400\n",
    "\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "sess = tf.InteractiveSession() # create the sesion object\n",
    "tf.global_variables_initializer().run() # initialize variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Algorithm: TensorFlow Automatically Takes Care of Gradient Calculations :) \n",
    "\n",
    "### TODO: Create a loop to run the training algorithm\n",
    "# Inside the loop:\n",
    "# 1) create variables batch_xs, and batch_ys, which will be our x training batch and y training batch. Use generate_batch()\n",
    "# 2) call sess.run() (info here: https://www.tensorflow.org/api_docs/python/tf/InteractiveSession#run)\n",
    "    # i) in sess.run(), the fetches argument will be the train_step defined above\n",
    "    # ii) The feed_dict will pass batch_xs for the x placeholder, and batch_ys for the y_ placeholder\n",
    "\n",
    "for i in range(iterations):\n",
    "    batch_xs, batch_ys = generate_batch(batch_size)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Evaluate the Model\n",
    "Now, let's see how accurrate our hypothesis function is on our test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9138\n"
     ]
    }
   ],
   "source": [
    "# Model Evaluation\n",
    "\n",
    "# store correct_predictions list and calculate accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) \n",
    "\n",
    "# look at predictions on our test dataset\n",
    "print(sess.run(accuracy, feed_dict={x: test_x, y_: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:caispp]",
   "language": "python",
   "name": "conda-env-caispp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
